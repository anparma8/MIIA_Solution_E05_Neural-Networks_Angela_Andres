{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ángela María Arias Cód. 201728551\n",
    "\n",
    "Raúl Andrés Pardo Moreno Cód. 201727367"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 05\n",
    "\n",
    "# Neural networks\n",
    "\n",
    "## 4.1 Little Red Riding Hood Network\n",
    "\n",
    "Train a neural network to solve the  Little Red Riding Hood problem in sklern and Keras. Try the neural networ with different inputs and report the results.\n",
    "\n",
    "________________\n",
    "\n",
    "## 4.2 Boston House Price Prediction\n",
    "\n",
    "In the next questions we are going to work using the dataset *Boston*. This dataset measures the influence of socioeconomical factors on the price of several estates of the city of Boston. This dataset has 506 instances, each one characterized by 13 features:\n",
    "\n",
    "* CRIM - per capita crime rate by town\n",
    "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS - proportion of non-retail business acres per town.\n",
    "* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX - nitric oxides concentration (parts per 10 million)\n",
    "* RM - average number of rooms per dwelling\n",
    "* AGE - proportion of owner-occupied units built prior to 1940\n",
    "* DIS - weighted distances to five Boston employment centres\n",
    "* RAD - index of accessibility to radial highways\n",
    "* TAX - full-value property-tax rate per 10,000 USD\n",
    "* PTRATIO - pupil-teacher ratio by town\n",
    "* B - $1000(Bk - 0.63)^2$ where $Bk$ is the proportion of blacks by town\n",
    "* LSTAT - % lower status of the population\n",
    "\n",
    "Output variable:\n",
    "* MEDV - Median value of owner-occupied homes in 1000's USD\n",
    "\n",
    "**Note:** In this exercise we are going to predict the price of each estate, which is represented in the `MEDV` variable. It is important to remember that we are always aiming to predict `MEDV`, no matter which explanatory variables we are using. That means, in some cases we will use a subset of the 13 previously mentioned variables, while in other cases we will use all the 13 variables. But in no case we will change the dependent variable $y$.\n",
    "\n",
    "\n",
    "\n",
    "1. Load the dataset using `from sklearn.datasets import load_boston`.\n",
    "2. Create a DataFrame using the attribute `.data` from the loading function of Scikit-learn.\n",
    "3. Assign the columns of the DataFrame so they match the `.feature_names` attribute from the loading function of Scikit-learn. \n",
    "4. Assign a new column to the DataFrame which holds the value to predict, that means, the `.target` attribute of the loading function of Scikit-learn. The name of this columns must be `MEDV`.\n",
    "5. Use the function `.describe()` from Pandas for obtaining statistics about each column.\n",
    "\n",
    "## 4.3 Feature analysis:\n",
    "\n",
    "Using the DataFrame generated in the previous section:\n",
    "* Filter the dataset to just these features:\n",
    "     * Explanatory: 'LSTAT', 'INDUS', 'NOX', 'RM', 'AGE'\n",
    "     * Dependent: 'MEDV'.\n",
    "* Generate a scatter matrix among the features mentioned above using Pandas (`scatter_matrix`) or Seaborn (` pairplot`).\n",
    "     * Do you find any relationship between the features?\n",
    "* Generate the correlation matrix between these variables using `numpy.corrcoef`. Also include `MEDV`.\n",
    "     * Which characteristics are more correlated?\n",
    "     * BONUS: Visualize this matrix as heat map using Pandas, Matplotlib or Seaborn.\n",
    "\n",
    "## 4.4 Modeling linear and non linear relationships\n",
    "\n",
    "* Generate two new subsets filtering these characteristics:\n",
    "     * $D_1$:  $X = \\textit{'RM'}$, $y = \\textit{'MEDV'}$\n",
    "     * $D_2$:  $X = \\textit{'LSTAT'}$, $y = \\textit{'MEDV'}$\n",
    "* For each subset, generate a training partition and a test partition using a ratio of $ 70 \\% - 30 \\% $\n",
    "* Train a linear regression model on both subsets of data:\n",
    "     * Report the mean square error on the test set\n",
    "     * Print the values of $ w $ and $ w_0 $ of the regression equation\n",
    "     * Generate a graph where you visualize the line obtained by the regression model in conjunction with the training data and the test data\n",
    "* How does the model perform on $ D_1 $ and $ D_2 $? Why?\n",
    "\n",
    "## 4.5 Training a regression model\n",
    "\n",
    "* Generate a 70-30 partitioning of the data **using all the features**. (Do not include the dependent variable `MEDV`)\n",
    "* Train a linear regression model with the objective of predicting the output variable `MEDV`.\n",
    "     * Report the mean square error on the test set\n",
    "* Train a regression model using `MLPRegressor` in order to predict the output variable` MEDV`.\n",
    "     * Report the mean square error on the test set\n",
    "* Scale the data so that they have zero mean variance one per feature (only $ X $). You can use the following piece of code:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "sc_x.fit(X)\n",
    "X_train_s = sc_x.transform(X_train)\n",
    "X_test_s = sc_x.transform(X_test)\n",
    "```\n",
    "Check more information about `StandardScaler` [here](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "* Train the following models:\n",
    "     1. Train a linear regression model using the scaled data.\n",
    "         * Report the mean square error on the test set\n",
    "     2. Train a regression model using a 2-layer MultiLayer Perceptron (128 neurons in the first and 512 in the second) and with the **scaled data**.\n",
    "         * Report the mean square error on the test set\n",
    "     3. Which model has better performance? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4.1 Little Red Riding Hood Network\n",
    "#Train a neural network to solve the Little Red Riding Hood problem in sklern and Keras. Try the neural networ with different inputs and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(4, 3, 4), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.1.SKlearn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "X = np.array([[1,1,0,0],[0,1,1,0],[0,0,0,1]])\n",
    "y = np.array([[1,0,0,0],[0,0,1,1],[0,1,1,0]])\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(4,3, 4))\n",
    "clf.fit(X, y)  \n",
    "\n",
    "\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#X = [[1,1,0,0],[0,1,1,0],[0,0,0,1]]\n",
    "#y = [[1,0,0,0],[0,0,1,1],[0,1,1,0]]\n",
    "\n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(4,3, 4))\n",
    "#clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([1,1,0,0]) #[1,0,0,0], resultado [1,0,0,0] coincide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([0,1,1,0]) #[0,0,1,1], resultado [0,0,1,1] coincide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([0,0,0,1]) #[0,1,1,0], resultado [0,1,1,0] coincide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([0,1,0,1]) #resultado [0,1,1,0] resultado lógico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harol\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([1,0,0,1]) #[0,1,1,0] resultado lógico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________\n",
      "Layer (type)                   Output Shape                Param #    \n",
      "======================================================================\n",
      "dense_3 (Dense)                (None, 40)                  200        \n",
      "______________________________________________________________________\n",
      "activation_3 (Activation)      (None, 40)                  0          \n",
      "______________________________________________________________________\n",
      "dense_4 (Dense)                (None, 4)                   164        \n",
      "______________________________________________________________________\n",
      "activation_4 (Activation)      (None, 4)                   0          \n",
      "======================================================================\n",
      "Total params: 364\n",
      "Trainable params: 364\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#4.1.Keras\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "X = np.array([[1,1,0,0],[0,1,1,0],[0,0,0,1]])\n",
    "y = np.array([[1,0,0,0],[0,0,1,1],[0,1,1,0]]) \n",
    "\n",
    "#Initializing Neural Network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=40, input_dim=4)) #tiene 4 entradas y una capa densa de 40 neuronas(todas las neuronas de la capa anterior se conectan con la capa actual, o sea 80) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=4)) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary(70)\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss = 'binary_crossentropy',  #funcion de perdida\n",
    "              optimizer = SGD(lr=0.01, momentum=0.99, decay=0.0, nesterov=False), #SGD es el algoritmo estocasico para optimizar. Lr es el tamaño del paso. Se espera que este paso se sea prudente y se puede ajustar. El momentum es el impulso del vector sobre la superficie de error \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 0.7633 - acc: 0.3333\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.7623 - acc: 0.4167\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.7604 - acc: 0.4167\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.7576 - acc: 0.4167\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.7539 - acc: 0.4167\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.7494 - acc: 0.4167\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.7441 - acc: 0.4167\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.7380 - acc: 0.4167\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.7312 - acc: 0.4167\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.7237 - acc: 0.4167\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.7157 - acc: 0.4167\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.7071 - acc: 0.4167\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.6980 - acc: 0.5000\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.6886 - acc: 0.5833\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.6788 - acc: 0.5833\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.6688 - acc: 0.6667\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.6587 - acc: 0.6667\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.6484 - acc: 0.6667\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.6385 - acc: 0.7500\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.6287 - acc: 0.7500\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.6189 - acc: 0.7500\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.6096 - acc: 0.7500\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.6005 - acc: 0.7500\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5913 - acc: 0.7500\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5825 - acc: 0.7500\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5741 - acc: 0.7500\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5660 - acc: 0.8333\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5580 - acc: 0.8333\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5501 - acc: 0.8333\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5427 - acc: 0.8333\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5353 - acc: 0.8333\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5279 - acc: 0.8333\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5205 - acc: 0.7500\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5131 - acc: 0.7500\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5059 - acc: 0.7500\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.4987 - acc: 0.7500\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.4915 - acc: 0.7500\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.4844 - acc: 0.7500\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.4772 - acc: 0.8333\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4702 - acc: 0.8333\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4634 - acc: 0.8333\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4566 - acc: 0.8333\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4498 - acc: 0.8333\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4429 - acc: 0.8333\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4357 - acc: 0.8333\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.4284 - acc: 0.8333\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4210 - acc: 0.8333\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4133 - acc: 0.8333\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4055 - acc: 0.8333\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.3975 - acc: 0.8333\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.3893 - acc: 0.8333\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.3812 - acc: 0.8333\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.3728 - acc: 0.8333\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.3642 - acc: 0.8333\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.3554 - acc: 0.8333\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.3463 - acc: 0.8333\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.3370 - acc: 0.8333\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.3275 - acc: 0.9167\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.3176 - acc: 1.0000\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.3076 - acc: 1.0000\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.2973 - acc: 1.0000\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.2869 - acc: 1.0000\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.2765 - acc: 1.0000\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.2660 - acc: 1.0000\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.2553 - acc: 1.0000\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.2446 - acc: 1.0000\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.2339 - acc: 1.0000\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.2231 - acc: 1.0000\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.2124 - acc: 1.0000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.2017 - acc: 1.0000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.1912 - acc: 1.0000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.1808 - acc: 1.0000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.1708 - acc: 1.0000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.1609 - acc: 1.0000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.1515 - acc: 1.0000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.1424 - acc: 1.0000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.1336 - acc: 1.0000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.1252 - acc: 1.0000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.1172 - acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.1095 - acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.1023 - acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0953 - acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0888 - acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0826 - acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0768 - acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0714 - acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0662 - acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0614 - acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0569 - acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0527 - acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0488 - acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0452 - acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0418 - acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0387 - acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0357 - acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0330 - acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0305 - acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0281 - acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0260 - acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0240 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2dc670d44e0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x=orejas grandes, dientes grandes, apuesto, arrugado\n",
    "#y=grito, abrazo, comida,beso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 0., 0., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#print(model.predict(np.array([[1,1,0,0]]))) #[1,0,0,0] coincide\n",
    "\n",
    "predictions = model.predict(np.array([[1,1,0,0]])) #[1,0,0,0] coincide\n",
    "rounded = [np.round_(predictions, 0)]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 1., 1.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#print(model.predict(np.array([[0,1,1,0]]))) #[0,0,1,1] coincide\n",
    "\n",
    "predictions = model.predict(np.array([[0,1,1,0]])) #[0,0,1,1] coincide\n",
    "rounded = [np.round_(predictions, 0)]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 1., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#print(model.predict(np.array([[0,0,0,1]]))) #[0,1,1,0] coincide\n",
    "\n",
    "predictions = model.predict(np.array([[0,0,0,1]])) #[0,1,1,0] coincide\n",
    "rounded = [np.round_(predictions, 0)]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 1., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#print(model.predict(np.array([[0,1,0,1]]))) #[0, 1, 1, 0] resultado lógico\n",
    "\n",
    "predictions = model.predict(np.array([[0,1,0,1]])) #[0,1,1,0] coincide\n",
    "rounded = [np.round_(predictions, 0)]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#print(model.predict(np.array([[1,0,0,1]]))) #[0, 1, 1, 0] resultado lógico\n",
    "\n",
    "predictions = model.predict(np.array([[1,0,0,1]])) #[0,1,1,0] coincide\n",
    "rounded = [np.round_(predictions, 0)]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
